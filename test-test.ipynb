{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d1ad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/m4/60ljkntj7gs489xq6yv_ghnm0000gn/T/ipykernel_76317/1295810478.py\", line 2, in <module>\n",
      "    import torch\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/safiaboutaleb/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from supabase import create_client, Client\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2bc4e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script Started\n"
     ]
    }
   ],
   "source": [
    "# load environment variables - getting secret key and url\n",
    "print(\"Script Started\") #test print to check if the script runs\n",
    "load_dotenv()\n",
    "\n",
    "SUPABASE_URL = os.getenv(\"VITE_SUPABASE_URL\")\n",
    "SUPABASE_SECRET_KEY = os.getenv(\"SUPABASE_SECRET_KEY\")  # must use service key\n",
    "\n",
    "if not SUPABASE_URL or not SUPABASE_SECRET_KEY:\n",
    "    raise ValueError(\"Missing SUPABASE_URL or SUPABASE_SECRET_KEY in .env\")\n",
    "\n",
    "# create supabase client\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb01dd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.data\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# --- Run it ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m results = \u001b[43mfind_top_matches\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpink-sweater.png\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(item[\u001b[33m\"\u001b[39m\u001b[33mclothing_name\u001b[39m\u001b[33m\"\u001b[39m], item[\u001b[33m\"\u001b[39m\u001b[33mimage_url\u001b[39m\u001b[33m\"\u001b[39m], item[\u001b[33m\"\u001b[39m\u001b[33mdistance\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mfind_top_matches\u001b[39m\u001b[34m(image_path, k)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_top_matches\u001b[39m(image_path, k=\u001b[32m3\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     query_embedding = \u001b[43mcompute_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     response = supabase.rpc(\n\u001b[32m     17\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmatch_embeddings\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# stored function name\u001b[39;00m\n\u001b[32m     18\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mquery_embedding\u001b[39m\u001b[33m\"\u001b[39m: query_embedding, \u001b[33m\"\u001b[39m\u001b[33mmatch_count\u001b[39m\u001b[33m\"\u001b[39m: k}\n\u001b[32m     19\u001b[39m     ).execute()\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.data\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcompute_embedding\u001b[39m\u001b[34m(image_path)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_embedding\u001b[39m(image_path):\n\u001b[32m      6\u001b[39m     image = Image.open(image_path).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     image = \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m.unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      9\u001b[39m         embedding = model.encode_image(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/eco-wardrobe/test-env/lib/python3.11/site-packages/torchvision/transforms/functional.py:167\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[32m    166\u001b[39m mode_to_nptype = {\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m: np.int32, \u001b[33m\"\u001b[39m\u001b[33mI;16\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.byteorder == \u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mI;16B\u001b[39m\u001b[33m\"\u001b[39m: np.int16, \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m: np.float32}\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m img = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pic.mode == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    170\u001b[39m     img = \u001b[32m255\u001b[39m * img\n",
      "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# --- Load CLIP ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def compute_embedding(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model.encode_image(image)\n",
    "    embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "    return embedding.cpu().numpy().tolist()[0]\n",
    "\n",
    "def find_top_matches(image_path, k=3):\n",
    "    query_embedding = compute_embedding(image_path)\n",
    "\n",
    "    response = supabase.rpc(\n",
    "        \"match_embeddings\",  # stored function name\n",
    "        {\"query_embedding\": query_embedding, \"match_count\": k}\n",
    "    ).execute()\n",
    "\n",
    "    return response.data\n",
    "\n",
    "# --- Run it ---\n",
    "results = find_top_matches(\"pink-sweater.png\")\n",
    "\n",
    "for item in results:\n",
    "    print(item[\"clothing_name\"], item[\"image_url\"], item[\"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535fffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
